{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DymOIqSaMyHiWnn0V6vp6otev2HbmOKU",
      "authorship_tag": "ABX9TyPlYde7lEqXSyLfC/4HYcol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIVEK818713/Feature-Engineering/blob/main/3_Column_Transformer_%2B_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ColumnTransformation"
      ],
      "metadata": {
        "id": "idu0zA6z4mG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45LMxtWI4gSy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "id": "xFkeGvmm4ufe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/0machine learning/csv data/covid.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0sr9bsDH4zpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now understand the data and after that use column transfromation on desired column"
      ],
      "metadata": {
        "id": "zVvZAmAR5DZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['city'].value_counts()"
      ],
      "metadata": {
        "id": "opeS1RE2491G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cough'].value_counts()"
      ],
      "metadata": {
        "id": "MLuXkfB55Gss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "rKkWUBxP5IzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "n3-6pivH5Ksy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## here we will not work on 'age' column. \n",
        "## we will apply nominal categorical encoding on 'cough' by using OrdinalEncoder\n",
        "## we will apply OneHotEncoding on column 'gender' and 'city' by using OneHotEncoder\n",
        "## you can do Label encoding on target column'has_covid' but we are not do it now in this topic.\n",
        "## and we will handle missing value using simpleimputer, we will discuss simple imputer later on. simple imputer is using for filling the missing value."
      ],
      "metadata": {
        "id": "ReoqD0gc5XQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['has_covid']),df['has_covid'],\n",
        "                                                test_size=0.2)"
      ],
      "metadata": {
        "id": "qfrRAZol5NLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##without using column transformation"
      ],
      "metadata": {
        "id": "lse4gyKe5daO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding simple imputer to fever col\n",
        "si = SimpleImputer()\n",
        "X_train_fever = si.fit_transform(X_train[['fever']])\n",
        "\n",
        "# also the test data\n",
        "X_test_fever = si.fit_transform(X_test[['fever']])\n",
        "                                 \n",
        "X_train_fever.shape  "
      ],
      "metadata": {
        "id": "bOhvpU8_5Zyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinalencoding -> cough\n",
        "oe = OrdinalEncoder(categories=[['Mild','Strong']])\n",
        "X_train_cough = oe.fit_transform(X_train[['cough']])\n",
        "\n",
        "# also the test data\n",
        "X_test_cough = oe.fit_transform(X_test[['cough']])\n",
        "\n",
        "X_train_cough.shape"
      ],
      "metadata": {
        "id": "LL6gNa4k5hQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OneHotEncoding -> gender,city\n",
        "ohe = OneHotEncoder(drop='first',sparse=False)\n",
        "X_train_gender_city = ohe.fit_transform(X_train[['gender','city']])\n",
        "\n",
        "# also the test data\n",
        "X_test_gender_city = ohe.fit_transform(X_test[['gender','city']])\n",
        "\n",
        "X_train_gender_city.shape"
      ],
      "metadata": {
        "id": "IkFMc7jz5lSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##we are removing 'age' column and convert it to nparray then concatenate it with all other columns"
      ],
      "metadata": {
        "id": "6Bmhln5U5tC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Age\n",
        "X_train_age = X_train.drop(columns=['gender','fever','cough','city']).values\n",
        "\n",
        "# also the test data\n",
        "X_test_age = X_test.drop(columns=['gender','fever','cough','city']).values\n",
        "\n",
        "X_train_age.shape"
      ],
      "metadata": {
        "id": "LOqoY6xM5n5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = np.concatenate((X_train_age,X_train_fever,X_train_gender_city,X_train_cough),axis=1)\n",
        "# also the test data\n",
        "X_test_transformed = np.concatenate((X_test_age,X_test_fever,X_test_gender_city,X_test_cough),axis=1)\n",
        "\n",
        "X_train_transformed.shape"
      ],
      "metadata": {
        "id": "OqkX_Hxp5w1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now using column Transformation"
      ],
      "metadata": {
        "id": "D356Aw2i53__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "QK6wl42N5zeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = ColumnTransformer(transformers=[\n",
        "    ('tnf1',SimpleImputer(),['fever']),\n",
        "    ('tnf2',OrdinalEncoder(categories=[['Mild','Strong']]),['cough']),\n",
        "    ('tnf3',OneHotEncoder(sparse=False,drop='first'),['gender','city'])\n",
        "],remainder='passthrough')"
      ],
      "metadata": {
        "id": "sRn-SPzN57xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "8BH4GdTD6Bb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "T421-KqQ6Ee_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformer.fit_transform(X_train).shape)\n",
        "print(transformer.transform(X_test).shape)"
      ],
      "metadata": {
        "id": "VZcfl4CV6Gn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline"
      ],
      "metadata": {
        "id": "g2yYa_Sb6mMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os  "
      ],
      "metadata": {
        "id": "4m2u6ZHm6Izd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']='/content/drive/MyDrive/Colab Notebooks/Kaggle json'"
      ],
      "metadata": {
        "id": "K7L01clY6rwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hesh97/titanicdataset-traincsv"
      ],
      "metadata": {
        "id": "s5uBKFt16t6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/titanicdataset-traincsv.zip"
      ],
      "metadata": {
        "id": "blQ5F3Ec6wBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-NbL1Q9H6ynm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "c-hqlS6X60g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##work without pipeline"
      ],
      "metadata": {
        "id": "MwJeZyO367Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "RhOHnWVo63gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##we are droping some column because these column is not needed for our analysis , these columns are 'PassengerId','Name','Ticket','Cabin'"
      ],
      "metadata": {
        "id": "4WkaWf_67DuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)  "
      ],
      "metadata": {
        "id": "boM4voHX6-0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "c_knkgts7Hso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now spilt our data into train and test split"
      ],
      "metadata": {
        "id": "hYKvIMeE7Rhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 -> train/test/split\n",
        "X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['Survived']),\n",
        "                                                 df['Survived'],\n",
        "                                                 test_size=0.2,\n",
        "                                                random_state=42)"
      ],
      "metadata": {
        "id": "WxuJZ2Q97NQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now Check whether there is any missing values or not in any columns"
      ],
      "metadata": {
        "id": "UmYJHtmS7ZpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "bFs9fn0w7Upz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##there are missing values in 'age' and 'Embarked' column so we are using simple imputer to fill this missing values. in age we are simpleimputer using 'mean' as default and 'embarked' column we use strategy 'most-frequent'."
      ],
      "metadata": {
        "id": "dcSnPAmL7jaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying imputation\n",
        "\n",
        "si_age = SimpleImputer()\n",
        "si_embarked = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_train_age = si_age.fit_transform(X_train[['Age']])\n",
        "X_train_embarked = si_embarked.fit_transform(X_train[['Embarked']])\n",
        "\n",
        "X_test_age = si_age.transform(X_test[['Age']])\n",
        "X_test_embarked = si_embarked.transform(X_test[['Embarked']])"
      ],
      "metadata": {
        "id": "0TWs5z3A7c5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embarked"
      ],
      "metadata": {
        "id": "M3LZ-GGF7nbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_age"
      ],
      "metadata": {
        "id": "sXsDhCp_7rVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now apply one hot encoding on column 'sex' and 'embarked'"
      ],
      "metadata": {
        "id": "zZmbcoqw76QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding Sex and Embarked\n",
        "\n",
        "ohe_sex = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "ohe_embarked = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "\n",
        "X_train_sex = ohe_sex.fit_transform(X_train[['Sex']])\n",
        "X_train_embarked = ohe_embarked.fit_transform(X_train_embarked)\n",
        "\n",
        "X_test_sex = ohe_sex.transform(X_test[['Sex']])\n",
        "X_test_embarked = ohe_embarked.transform(X_test_embarked)"
      ],
      "metadata": {
        "id": "tHaJeY-R7tgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embarked"
      ],
      "metadata": {
        "id": "Nt5U3cM_7-L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(2)"
      ],
      "metadata": {
        "id": "Fgqa32oY8EOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we have 3 array namely x_train_age,x_train_sex, and x-train_embarked. and we need rest of columns as it is so we will create a array in which we have rest of columns. \n",
        "## for that we will drop 3 columns sex,age,embarked because we have now 3 new array for this columns"
      ],
      "metadata": {
        "id": "DvhVDA3R8N-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rem = X_train.drop(columns=['Sex','Age','Embarked'])"
      ],
      "metadata": {
        "id": "r_82OE168GMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_rem = X_test.drop(columns=['Sex','Age','Embarked'])"
      ],
      "metadata": {
        "id": "50xMSpA58Rpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##now concatenate all 4 array X_train_rem,x-train_age,x_train_sex, and x-train_embarked"
      ],
      "metadata": {
        "id": "XxmNvI6v8X1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = np.concatenate((X_train_rem,X_train_age,X_train_sex,X_train_embarked),axis=1)\n",
        "X_test_transformed = np.concatenate((X_test_rem,X_test_age,X_test_sex,X_test_embarked),axis=1)  "
      ],
      "metadata": {
        "id": "MwIKHl8k8TjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed"
      ],
      "metadata": {
        "id": "FrSh48Em8bvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now use decision tree"
      ],
      "metadata": {
        "id": "316ZuWtN8huL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train_transformed,y_train)"
      ],
      "metadata": {
        "id": "Xa1AKuuE8d5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test_transformed)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "l8fPinYX8koM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now check accuracy"
      ],
      "metadata": {
        "id": "sh1y21HK8qBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "59AEANQq8mf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##above we did not do column transformation that's why our work is too lenghty. we pick each column and do the desired work.\n",
        "##now assume that we have a html form where we can enter new passanger data and our model will predict that passanger will survived or not."
      ],
      "metadata": {
        "id": "eKKynJdt8yZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##for exporting the model we have to import pickle"
      ],
      "metadata": {
        "id": "V8POsjJa83-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle  "
      ],
      "metadata": {
        "id": "54l7f2Vo8tpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##we have to use pickle.dump(classifer name,open(location,mode)\n",
        "##and we have to use ohe_sex and ohe_embarked also because at the run time when new data is given by user then he/she will enter 'male/female' for sex entry and emabrked value like's','c' or 'q' .\n",
        "##so we have to do one hot encoding of this entry data that's why we are using ohe_sex and ohe_embarked ."
      ],
      "metadata": {
        "id": "WDEuUVta8_7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe_sex,open('ohe_sex.pkl','wb'))\n",
        "pickle.dump(ohe_embarked,open('ohe_embarked.pkl','wb'))\n",
        "pickle.dump(clf,open('clf.pkl','wb'))  "
      ],
      "metadata": {
        "id": "FNMvhm_J88Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##But how to use these model?\n",
        "##lets consider we have a page where we have to give data and predict the output.\n",
        "##for this purpose we will use another step of coding in this file as below."
      ],
      "metadata": {
        "id": "GgbZxbEL9Ly9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M1SmyumE9Fus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##now load the model. as you encode here we will decode .\n",
        "##use pickle.load()"
      ],
      "metadata": {
        "id": "J513Ke1I9UVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_sex = pickle.load(open('ohe_sex.pkl','rb'))\n",
        "ohe_embarked = pickle.load(open('ohe_embarked.pkl','rb'))\n",
        "clf = pickle.load(open('clf.pkl','rb'))"
      ],
      "metadata": {
        "id": "ZepSgPlO9RB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume user input\n",
        "# Pclass/gender/age/SibSp/Parch/Fare/Embarked\n",
        "test_input = np.array([2, 'male', 31.0, 0, 0, 10.5, 'S'],dtype=object).reshape(1,7)"
      ],
      "metadata": {
        "id": "NDda2fd19ZoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input"
      ],
      "metadata": {
        "id": "snynqAci9cUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##here we have to do all the preprocessing steps as we done previously like imputer,onehot encoder then concatenate and then prediction. here you should not do imputer beacause data is provided by you and there is no missing value."
      ],
      "metadata": {
        "id": "8RpsSii49kpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_sex = ohe_sex.transform(test_input[:,1].reshape(1,1))"
      ],
      "metadata": {
        "id": "2k4QwyFY9hCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_sex\n",
        "# output 0,1 because it is a 'male'"
      ],
      "metadata": {
        "id": "hoNTYZqi9pnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_embarked = ohe_embarked.transform(test_input[:,-1].reshape(1,1))"
      ],
      "metadata": {
        "id": "c61oo7Rt9r32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_embarked"
      ],
      "metadata": {
        "id": "wsU3h8I59t07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_age = test_input[:,2].reshape(1,1)"
      ],
      "metadata": {
        "id": "zVjvnN2Q9wa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_age"
      ],
      "metadata": {
        "id": "BL3do-zU9yXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##now concatenate rest columns with test_input_age,test_input_sex,test_input_embarked."
      ],
      "metadata": {
        "id": "1rMbJbPd97jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_transformed = np.concatenate((test_input[:,[0,3,4,5]],test_input_age,test_input_sex,test_input_embarked),axis=1)"
      ],
      "metadata": {
        "id": "aUlTsPsB90sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_transformed"
      ],
      "metadata": {
        "id": "ftE3NBtW9_uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_transformed.shape"
      ],
      "metadata": {
        "id": "T61fCREC-B-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now predict"
      ],
      "metadata": {
        "id": "tRsAuyu7-IZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(test_input_transformed)"
      ],
      "metadata": {
        "id": "owPC_IYH-ECo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##above code is your production code. you can think we have do alot of work for that\n",
        "##but but but if we use pipeline then this work will be reduced in the next section we will use pipeline.\n",
        "##understand the thing if you are not using pipeline ...\n",
        "##steps of series you are doing in the traing time that series of steps you have write in your production code. if change a little bit in traning part then you have to change in production also.\n",
        "##now lets see if you used pipeline what will happen.."
      ],
      "metadata": {
        "id": "h1lnSnTl-Q3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##titanic-using-pipeline"
      ],
      "metadata": {
        "id": "W8ZYXtkq-eqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "IvZ1uNaZ-LfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn. feature_selection import SelectKBest,chi2\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "8-zjpGFG-kK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']='/content/drive/MyDrive/Colab Notebooks/Kaggle json'"
      ],
      "metadata": {
        "id": "dkeMx_h1-l9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hesh97/titanicdataset-traincsv"
      ],
      "metadata": {
        "id": "h9G2z3IN-ort"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/titanicdataset-traincsv.zip"
      ],
      "metadata": {
        "id": "goqojZ1w-qkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "u35FBWQZ-tcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)"
      ],
      "metadata": {
        "id": "zlNEv7aM-zor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How we will make our pipeline ? What is our Plan?\n",
        "##The first step of our pipeline is impute the missing value in age and embarked using one column transform , the out put of this we will use an input to next column transform which work on onehot encoding on column sex and embarked. The output of this will be input to next column transformer which is scaling(it may be it is useful or not but for understanding of pipeline we will do this step). The out of this column transformer will be the input to next step which is feature selcetion(means we will take best 5 or 8 feature for our consideration, it may be result goes worst but for understanding pipeline we including this beacuse it may be in any case you need it in future).\n",
        "##After that we will train our model using Decision tree algorithm."
      ],
      "metadata": {
        "id": "qtSGVUxZ_EA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.common import random_state\n",
        "X_train,X_test,y_train,y_test=train_test_split(df.drop(columns=['Survived']),df['Survived'],\n",
        "                                               test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "En6shFxS--UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##first column transformer creation for missing value imputation\n",
        "##we use index no. instead of column name because output of this transformer is not a dataframe but it is an array so you know that numpy don't understand column name it understand only index no.\n",
        "##we don't want to drop rest columns so we use remainder='passthrough'"
      ],
      "metadata": {
        "id": "jEFBfrdm_OZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf1=ColumnTransformer([\n",
        "    ('Impute_Age',SimpleImputer(),[2]),\n",
        "    ('Impute_Embarked',SimpleImputer(strategy='most_frequent'),[6])\n",
        "],remainder='passthrough')"
      ],
      "metadata": {
        "id": "UA5_o_10_Kf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Next column transformer for one hot encoding on sex and embarked, we are not using drof first parameter because we are working on Decision tree"
      ],
      "metadata": {
        "id": "rPdbbhyJ_Z8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf2=ColumnTransformer([\n",
        "    ('ohe_sex_embarked',OneHotEncoder(sparse=False,handle_unknown='ignore'),[1,6])\n",
        "],remainder='passthrough')"
      ],
      "metadata": {
        "id": "u6ciXnNc_ZBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our next column transformer is scaling. here we are not using standered scalere because we will do feature selection in the next step and in feature seection we use Min Max Scaler.\n",
        "## we are appling MinMax scaler on all the columns if you want to skip categorical column you can do it but we are taking all columns in this case.\n",
        "##so we use slice(0,10) slice will apply minmax on this range. but why 0 to 10\n",
        "## understand this we have 7 columns\n",
        "## we do one hot encoding on age so it will give 2 new columns\n",
        "## and embarked will produce 3 new columns\n",
        "##**total 5 new columns**\n",
        "## but we have to remove age and embarked from the 7 columns so we have here 5 columns \n",
        "## so **5 plus 5 is 10**"
      ],
      "metadata": {
        "id": "lMKYgotp_qTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf3=ColumnTransformer([\n",
        "    ('scale',MinMaxScaler(),slice(0,10))\n",
        "])"
      ],
      "metadata": {
        "id": "RRYLxAGG_lGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##next step is feature selection\n",
        "##we dont need to apply column transformer\n",
        "##here you should understand how you can insert feature selection step in your pipeline .here we are nor work on how feature selection work."
      ],
      "metadata": {
        "id": "_AJUCIZt_5ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf4=SelectKBest(score_func=chi2,k=8)"
      ],
      "metadata": {
        "id": "jqsExeZQ_y7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model"
      ],
      "metadata": {
        "id": "74NlGB92ACUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf5=DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "pur62lYd_-pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now Create the Pipeline"
      ],
      "metadata": {
        "id": "L-E4Y1KFAJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=Pipeline([\n",
        "    ('trf1',trf1),\n",
        "    ('trf2',trf2),\n",
        "    ('trf3',trf3),\n",
        "    ('trf4',trf4),\n",
        "    ('trf5',trf5)\n",
        "])"
      ],
      "metadata": {
        "id": "TSzBtg-AAF3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now we have our pipeline object and now we have to fit it on X_train and y_train\n",
        "##how pipeline work on X_train. first X_train goes to trf1 and output go to trf2 ,then its output go to trf3, then its output go to trf4 then its output go to trf5.\n",
        "##if in your pipe line there is no model trainig then you will use pipe.fit_transform because we are not tarin our model but in our case we are tarining our model so we use pipe.fit(X_train,y_train). so that we can call predict()."
      ],
      "metadata": {
        "id": "Xc69NErbARnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "fIweoAcZAOVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##if you want to display your pipeline in a good mannaer then use below code and then fit it"
      ],
      "metadata": {
        "id": "-yLfOVHoAdPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "t5ud90nKAYTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "9jmam5B_Agko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now understand how it works and how you can extract information"
      ],
      "metadata": {
        "id": "5pDYWKJiAoWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps"
      ],
      "metadata": {
        "id": "lQjw9cX8Ai2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1']"
      ],
      "metadata": {
        "id": "OmK8dALbAtGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_"
      ],
      "metadata": {
        "id": "imf0W2aYAvGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0]"
      ],
      "metadata": {
        "id": "W7D068ILAw8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0][1]"
      ],
      "metadata": {
        "id": "YWxA6Dm3Ay2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0][1].statistics_"
      ],
      "metadata": {
        "id": "S2xgjKuWA1An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to check\n",
        "pipe.named_steps['trf1'].transformers_[1][1].statistics_ "
      ],
      "metadata": {
        "id": "39HOGMRpA3Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "02gk2oZ3A5_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "ChksfhCCA8j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "M10rjPW9A-pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##accuracy is down it may be because of feature selection step you can chek it by removing feature seletion step from the pipeline it may it works.\n",
        "#cross validation using pipeline\n",
        "##it may be you dont know about cross validation but here we are doing a complete job related to pipeline. it may be you use cross validation in future so here we are adding these below stpes also for the understanding purpose."
      ],
      "metadata": {
        "id": "SOzJLpw4BGsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(pipe,X_train,y_train,cv=5,scoring='accuracy').mean()"
      ],
      "metadata": {
        "id": "KsYCwntNBAy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#gridsearch using pipeline\n",
        "##in the future you will use a term namely hyper parameter tuning means you will tune your parameter for the best value of the parameter so we use gridsearch.\n",
        "##in our case we are using Decision tree so we are using max depth parameter and tune it for best value. lets check it our model name is trf5."
      ],
      "metadata": {
        "id": "l6CV02VDBQDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param={\n",
        "    'trf5__max_depth':[1,2,3,4,5,None]\n",
        "}"
      ],
      "metadata": {
        "id": "ELy_EW-lBMNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid=GridSearchCV(pipe,param,cv=5,scoring='accuracy')\n",
        "grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "d8BMKF6JBlaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_score_"
      ],
      "metadata": {
        "id": "t8ub3swVBnTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "id": "6aLcT9R5Bpu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#exporting the pipeline\n",
        "##what if we want to use this pipeline in production so step is here"
      ],
      "metadata": {
        "id": "Y5NMz5SfDrZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(pipe,open('pipe.pkl','wb'))"
      ],
      "metadata": {
        "id": "Ybs7mp4EDkGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##now code for production"
      ],
      "metadata": {
        "id": "49t962-2D0yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "H9rEW-6YDwrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pickle.load(open('pipe.pkl','rb'))"
      ],
      "metadata": {
        "id": "xxcTJI6sD5bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=np.array([2,'male',31.0,0,0,10.5,'S'],dtype=object).reshape(1,7)"
      ],
      "metadata": {
        "id": "lcUFbuTtD75x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.predict(test_data)"
      ],
      "metadata": {
        "id": "Y8V8b-GcD-Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#in future if you change any coding then you need not to change the production code it will reamin unchanged.\n",
        "##you have change your code and udate your new pickle file on the server after modification in your file. your production code will be remain unchanged."
      ],
      "metadata": {
        "id": "gEVEAONUEHcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "DqZS2hWuEAfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}